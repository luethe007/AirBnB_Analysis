{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirBnB Munich - Analysis of Prices and Customer Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "bMlAVfW9aHTm",
    "outputId": "ad74dbe8-255c-42b1-b29f-791771c199f7"
   },
   "outputs": [],
   "source": [
    "# Read in necessary libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and wrangle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qz9YwbG9XojA"
   },
   "outputs": [],
   "source": [
    "# data downloaded 31/07/19, week 31\n",
    "df_calendar = pd.read_csv(\"C:/Users/Marvin/Documents/Data Science/Udacity/Data_Scientist_Nanodegree/7_Introduction_to_Data_Science/Project_Blog_Post/calendar.csv\")\n",
    "df_listings_summary = pd.read_csv('C:/Users/Marvin/Documents/Data Science/Udacity/Data_Scientist_Nanodegree/7_Introduction_to_Data_Science/Project_Blog_Post/listings_summary.csv')\n",
    "df_reviews = pd.read_csv('C:/Users/Marvin/Documents/Data Science/Udacity/Data_Scientist_Nanodegree/7_Introduction_to_Data_Science/Project_Blog_Post/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6Si1uPzXojP"
   },
   "outputs": [],
   "source": [
    "# clean the prices and return float values\n",
    "def clean_price(x):\n",
    "    x = x.replace(\",\", \"\")\n",
    "    return float(x[1:])\n",
    "\n",
    "# convert \"available\" column to booleans\n",
    "def clean_boolean(x):\n",
    "    if x == \"t\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# convert and clean\n",
    "df_calendar['price'] = df_calendar['price'].apply(clean_price)\n",
    "df_calendar['adjusted_price'] = df_calendar['adjusted_price'].apply(clean_price)\n",
    "df_calendar['available'] = df_calendar['available'].apply(clean_boolean)\n",
    "\n",
    "# convert string to datetime\n",
    "df_calendar['date'] = pd.to_datetime(df_calendar['date'])\n",
    "df_reviews[\"date\"] = pd.to_datetime(df_reviews['date'])\n",
    "\n",
    "# create month column\n",
    "df_calendar['Calendar Month'] = df_calendar['date'].dt.month\n",
    "df_calendar['Calendar Week'] = df_calendar['date'].dt.week\n",
    "\n",
    "df_calendar.rename(columns={\"price\": \"Price/Night (€)\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = df_calendar['Price/Night (€)']\n",
    "S[~((S-S.mean()).abs() > 3*S.std())].hist(bins = 100, color='#FF6666')\n",
    "plt.axvline(x=S.median(), color='k', linestyle='--')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Price/Night (€)')\n",
    "plt.title('Munich AirBnB Prices (June 2019 - July 2020)', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "colab_type": "code",
    "id": "vmrbaEp6XojS",
    "outputId": "b44f6897-b871-440c-8152-1f28ecb6b5c4"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.boxplot(x=\"Calendar Week\", y=\"Price/Night (€)\", data = df_calendar, showfliers=False, color=\"seagreen\").set_title(\"Munich AirBnB Price Boxplots per CW\",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9e9tjU3XojV"
   },
   "outputs": [],
   "source": [
    "october_festival = np.array(df_calendar[(df_calendar['Calendar Week']>37) & (df_calendar['Calendar Week']<41)]['Price/Night (€)'])\n",
    "rest = np.array(df_calendar[(df_calendar['Calendar Week']<38) | (df_calendar['Calendar Week']>40)]['Price/Night (€)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3Ehz5Gg8zBLQ",
    "outputId": "379d1d87-28a0-4e8b-9d5f-c097fb5191f3"
   },
   "outputs": [],
   "source": [
    "results = stats.ttest_ind(october_festival, rest, equal_var=False)\n",
    "alpha = 0.05\n",
    "if (results[0] > 0) & (results[1]/2 < alpha):\n",
    "    print(\"reject null hypothesis, AirBnB prices of {} are greater than during {}\".format('October Festival','rest of the year'))\n",
    "    print(\"p-value: \" + str(results[1]/2))\n",
    "    print(\"t-stat: \" + str(results[0]))\n",
    "else:\n",
    "    print(\"accept null hypothesis\")\n",
    "    print(\"p-value: \" +str(results[1]/2))\n",
    "    print(\"t-stat: \" + str(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiqXCz9qXojd"
   },
   "outputs": [],
   "source": [
    "# join df_calendar and df_reviews to combine price and review information\n",
    "df_calendar = df_calendar.groupby('listing_id').mean().reset_index()\n",
    "df_calendar = df_calendar[[\"listing_id\", \"Price/Night (€)\"]]\n",
    "merged = pd.merge(df_calendar, df_reviews, how='inner', on='listing_id')\n",
    "merged = pd.merge(merged, df_listings_summary, how='inner', left_on='listing_id', right_on='id')\n",
    "\n",
    "merged[\"review_length\"] = merged[\"comments\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qmh2aUdXojt"
   },
   "outputs": [],
   "source": [
    "documents = merged['comments'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "wddAdWgiXojz",
    "outputId": "bc7400cf-cfd6-4754-ac27-65537363e673"
   },
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "  \n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "stop = set(stopwords.words('english'))\n",
    "word_set = set(words.words())\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop and i in word_set])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split() if not hasNumbers(word) and len(word) > 1)\n",
    "    return normalized\n",
    "\n",
    "documents = [clean(doc) for doc in documents]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEX-ltryXoj2"
   },
   "outputs": [],
   "source": [
    "# src: https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "\n",
    "'''\n",
    "Data Preprocessing:\n",
    "- Tokenization: Split text into words, lowercase and remove punctuation \n",
    "- stopwords removed\n",
    "- words lemmatized: words in third person are changed to first person, verbs in past and future are changed into present\n",
    "\n",
    "'''\n",
    "spacy.load(\"en\")\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "  \n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1SdGuUJgXoj5",
    "outputId": "89c058e5-b641-4404-d581-e76a51ae982e"
   },
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "text_data = []\n",
    "for doc in documents:\n",
    "    tokens = prepare_text_for_lda(doc)\n",
    "    text_data.append(tokens)\n",
    "  \n",
    "bigram = Phrases(text_data, min_count = 20)\n",
    "trigram = Phrases(bigram[text_data], min_count = 20)  \n",
    "  \n",
    "for idx in range(len(text_data)):\n",
    "    for token in bigram[text_data[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            text_data[idx].append(token)\n",
    "    for token in trigram[text_data[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a trigram, add to document.\n",
    "            text_data[idx].append(token)\n",
    "\n",
    "keep_row = []\n",
    "for k in text_data:\n",
    "    if not k:\n",
    "        keep_row.append(False)\n",
    "    else:\n",
    "        keep_row.append(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [i for (i, v) in zip(text_data, keep_row) if v]\n",
    "merged = merged[keep_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTPqmiEkXoj7"
   },
   "outputs": [],
   "source": [
    "remove = [\"wonderful\", \"beautiful\", \"fantastic\", \"absolutely\", \"thoughtful\", \"getting\", \"extremely\", \"definitely\", \"willing\", \"super\", \"great\", \"really\", \"recommend\", \"would\", \"thank\", \"highly\", \"amaze\", \"perfect\", \"lovely\", \"welcome\", \"everything\", \"helpful\", \"ideal\", \"seine\", \"toller\", \"johannes\", \"nett\", \"excelente\", \"excellent\", \"value\", \"provide\", \"awesome\", \"strongly\"]\n",
    "remove = list(set(remove))\n",
    "for text in text_data:\n",
    "    for word in remove:\n",
    "        while True:\n",
    "            if word in text:\n",
    "                text.remove(word)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contain dict from text_data containing number of times a word appears\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "\n",
    "# filter out tokens that appear in less than 20 docs, in more than 0.8 docs (fraction), keep the first 100 000 most frequent tokens\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.8, keep_n=100000)\n",
    "\n",
    "# for each doc create dict reporting how many words and how many times those words appear (bag of words)  \n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('C:/Users/Marvin/Documents/Data Science/Udacity/Data_Scientist_Nanodegree/7_Introduction_to_Data_Science/Project_Blog_Post/corpus.pkl', 'wb'))\n",
    "dictionary.save('C:/Users/Marvin/Documents/Data Science/Udacity/Data_Scientist_Nanodegree/7_Introduction_to_Data_Science/Project_Blog_Post/dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BLBfHM1EXokC",
    "outputId": "3fcd850c-3b36-444b-a9fc-4846a4276da3"
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "# create LDA model, number of passes is number of training passes over document\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes = 15)\n",
    "ldamodel.save('C:/Users/Marvin/Documents/Data Science/Udacity/Data_Scientist_Nanodegree/7_Introduction_to_Data_Science/Project_Blog_Post/ldamodel.gensim')\n",
    "\n",
    "\n",
    "# sort_topic false means that gensim topic ordering is equal to pyLDAvis topic ordering (difference gensim starts at 0, pyLDAvis at 1)\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(lda_display, 'C:/Users/Marvin/Documents/Data Science/Udacity/Data_Scientist_Nanodegree/7_Introduction_to_Data_Science/Project_Blog_Post/lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LDA Model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T55GkZ3Y3_Qd"
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel.load('C:/Users/Marvin/Documents/Data Science/Udacity/Data_Scientist_Nanodegree/7_Introduction_to_Data_Science/Project_Blog_Post/ldamodel.gensim')\n",
    "dictionary = gensim.corpora.Dictionary.load('C:/Users/Marvin/Documents/Data Science/Udacity/Data_Scientist_Nanodegree/7_Introduction_to_Data_Science/Project_Blog_Post/dictionary.gensim')\n",
    "\n",
    "# get topic distribution for given doc\n",
    "labels = []\n",
    "for doc in text_data:\n",
    "    topics = ldamodel.get_document_topics(dictionary.doc2bow(doc))\n",
    "    prob = 0\n",
    "    for topic in topics:\n",
    "        if topic[1] > prob:\n",
    "            prob = topic[1]\n",
    "            label = topic[0]\n",
    "    if prob > 0.5:\n",
    "        labels.append(label + 1)\n",
    "    else:\n",
    "        labels.append(None)\n",
    "\n",
    "merged['Topics'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = merged.groupby(['Topics'])['Price/Night (€)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(grouped))\n",
    "ax1 = plt.subplot(1,1,1)\n",
    "w = 0.3\n",
    "#plt.xticks(), will label the bars on x axis with the respective country names.\n",
    "plt.xticks(x + w /2, grouped.index.astype(int))\n",
    "count =ax1.bar(x, grouped['count'], width=w, color='b', align='center')\n",
    "plt.ylabel('Reviews Count')\n",
    "plt.xlabel('Topics')\n",
    "# ax1.grid(False)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.grid(False)\n",
    "# mean prices per topic\n",
    "price =ax2.bar(x + w, grouped['mean'] , width=w,color='g',align='center')\n",
    "#Set the Y axis label as average prices.\n",
    "plt.ylabel('Average Prices')\n",
    "#To set the legend on the plot we have used plt.legend()\n",
    "plt.legend([count, price],['Reviews Count', 'Average Prices'])\n",
    "#To show the plot finally we have used plt.show().\n",
    "plt.title('Reviews Frequency vs Average Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'room_type'\n",
    "\n",
    "for i in range(1,11):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    baseline = pd.value_counts(merged[value])\n",
    "    topic_specific = pd.value_counts(merged[merged['Topics']==i][value])\n",
    "\n",
    "    baseline_ratio = baseline / baseline.sum()\n",
    "    topic_specific_ratio = topic_specific / topic_specific.sum()\n",
    "\n",
    "    topic_specific_ratio.subtract(baseline_ratio).dropna().sort_values(ascending = False)[:10].plot.bar()\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Topic ' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Latitude and longitude on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_df = (merged.groupby(['latitude', 'longitude']).size() \n",
    "   .sort_values(ascending=False) \n",
    "   .reset_index(name='count') \n",
    "   .drop_duplicates(subset='longitude'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmplot import gmplot\n",
    "max_lat = lat_long_df['latitude'].max()\n",
    "min_lat = lat_long_df['latitude'].min()\n",
    "max_lon = lat_long_df['longitude'].max()\n",
    "min_lon = lat_long_df['longitude'].min()\n",
    "\n",
    "mymap = gmplot.GoogleMapPlotter(\n",
    "    min_lat + (max_lat - min_lat) / 2, \n",
    "    min_lon + (max_lon - min_lon) / 2, \n",
    "    16)\n",
    "mymap.heatmap(lat_long_df['latitude'], lat_long_df['longitude'])\n",
    "\n",
    "mymap.draw('my_gm_plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['neighbourhood'].value_counts()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_df = lat_long_df[:int(lat_long_df.shape[0]*0.2)]\n",
    "keep_row = []\n",
    "for x, y in zip(merged['longitude'], merged['latitude']):\n",
    "    keep = False\n",
    "    for xi, yi in zip(lat_long_df['longitude'], lat_long_df['latitude']):\n",
    "        if (xi == x) and (yi == y):\n",
    "            keep = True\n",
    "            break\n",
    "    keep_row.append(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[keep_row].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
